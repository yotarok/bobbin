{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use `tpmap`\n",
    "\n",
    "This notebook demonstrates how to use `tpmap` and other miscelleneous utility functions supporting `pmap`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble: Install prerequisites, import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade pip\n",
    "!pip -q install --upgrade \"jax[cpu]\"\n",
    "!pip -q install git+https://github.com/yotarok/bobbin.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import functools\n",
    "\n",
    "import bobbin\n",
    "import chex\n",
    "import flax\n",
    "import jax\n",
    "import jax.experimental.host_callback as hcb\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "Array = chex.Array\n",
    "# Simulate multi-device environment by CPU\n",
    "chex.set_n_cpu_devices(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tpmap`\n",
    "\n",
    "`tpmap` is a thin-wrapper for pmap that attaches argument and return-value processings for ensuring transparent API.\n",
    "Basically, `tpmap` introduces some mechanism to inject argument translators and a return value translator to `jax.pmap` so `pmap`-ed function doesn't change their input/ output shapes.\n",
    "There's additional information needed because normal python functions do not know whether an argument is a data array that should be split, or a parameter array that should be distributed to all the devices.\n",
    "`tpmap` provides easier supports for this kind of parameter/ return-value translation.\n",
    "\n",
    "Let's first define the function to be parallelized.  The function below performs matrix multiplication and adds some noise to the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_parallel_noisy_matmul(matrix, xs, rng):\n",
    "    noise = 0.1 * jax.random.normal(rng, shape=(1, matrix.shape[-1]))\n",
    "    device_id = jax.lax.axis_index(\"i\")\n",
    "    hcb.id_print(device_id, shape_of_parameter=matrix.shape, shape_of_data=xs.shape)\n",
    "    return jnp.dot(xs, matrix) + noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there's some `hcb.*` calls for debugging purpose.\n",
    "\n",
    "The function can be transformed into data-parallel function by applying `tpmap` operator as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bobbin.tpmap(\n",
    "    data_parallel_noisy_matmul,\n",
    "    axis_name=\"i\",\n",
    "    argtypes=(\"broadcast\", \"shard\", \"rng\"),\n",
    "    wrap_return=lambda x: x.reshape((-1, x.shape[-1])),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `argtypes` specifies how to distribute the arguments. Each value has the following instructions.\n",
    "\n",
    "- \"broadcast\": Copy the argument to all the devices involved.\n",
    "- \"shard\": Split the leading axis (batch) by the number of devices and pass each shard to each device.\n",
    "- \"rng\": Split the RNG given as the argument to N child-RNGs and distribute child RNG to each device.\n",
    "\n",
    "In addition to the above used values, the following options can be used:\n",
    "\n",
    "- \"thru\": The argument is expected to have a device-axis so the argument is directly passed to the `pmap`-ed function.\n",
    "- \"static\": The argument is assumed to be a static argument that will be broadcasted.\n",
    "\n",
    "Furthermore, the method to handle return values is specified as `wrap_return` argument.  In this case, each device returns `(batch_size // device_count, output_dim)`-shaped array, and the default return shape of this function is `(device_count, batch_size // device_count, output_dim)`.  `wrap_return` argument specified above reshapes it back to `(batch_size, output_dim)` so we can ensure the same shape information as the original function.\n",
    "\n",
    "By calling `tpmap`-ed function as below, you will see that each function call is performed on a different device, and getting only a part of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape_of_data: (2, 8) shape_of_parameter: (8, 5)\n",
      "2\n",
      "shape_of_data: (2, 8) shape_of_parameter: (8, 5)\n",
      "7\n",
      "shape_of_data: (2, 8) shape_of_parameter: (8, 5)\n",
      "5\n",
      "shape_of_data: (2, 8) shape_of_parameter: (8, 5)\n",
      "4\n",
      "shape_of_data: (2, 8) shape_of_parameter: (8, 5)\n",
      "3\n",
      "shape_of_data: (2, 8) shape_of_parameter: (8, 5)\n",
      "6\n",
      "shape_of_data: (2, 8) shape_of_parameter: (8, 5)\n",
      "0\n",
      "shape_of_data: (2, 8) shape_of_parameter: (8, 5)\n",
      "1\n",
      "Result shape = (16, 5)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "input_dim = 8\n",
    "output_dim = 5\n",
    "\n",
    "parameter = np.random.normal(size=(input_dim, output_dim))\n",
    "data = np.random.normal(size=(batch_size, input_dim))\n",
    "result = f(parameter, data, jax.random.PRNGKey(0))\n",
    "print(f\"Result shape = {result.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscelleneous pmap utilities\n",
    "\n",
    "Besides `tpmap`, bobbin introduces some convenient tools around pmap.\n",
    "\n",
    "### `unshard`\n",
    "\n",
    "`unshard` is useful when to switch from JIT-ed multi-device operation to pure-Python CPU (and single-host) operation.\n",
    "The following example does dice rolling on each device and gathers counts as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(roll_count=Array([10, 10, 10, 10, 10, 10, 10, 10], dtype=int32, weak_type=True), six_count=Array([0., 4., 0., 0., 1., 1., 1., 3.], dtype=float32))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Result(flax.struct.PyTreeNode):\n",
    "    roll_count: Array\n",
    "    six_count: Array\n",
    "\n",
    "\n",
    "@functools.partial(bobbin.tpmap, axis_name=\"d\", argtypes=(\"static\", \"rng\"))\n",
    "def roll_dice(count, rng):\n",
    "    value = jax.random.randint(rng, shape=(count,), minval=1, maxval=7)\n",
    "    return Result(\n",
    "        six_count=(value == 6).astype(np.float32).sum(), roll_count=jnp.full((), count)\n",
    "    )\n",
    "\n",
    "\n",
    "results = roll_dice(10, jax.random.PRNGKey(0))\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we didn't set `wrap_return`, the result is a raw sharded representation from `pmap` that has a leading axis corresponding to each device.\n",
    "`unshard` is useful for such pytrees, if we want to do separate processing for each shard as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device #0 is normal.\n",
      "Device #1 was lucky! six-rate=40%!!\n",
      "Device #2 is normal.\n",
      "Device #3 is normal.\n",
      "Device #4 is normal.\n",
      "Device #5 is normal.\n",
      "Device #6 is normal.\n",
      "Device #7 is normal.\n"
     ]
    }
   ],
   "source": [
    "def print_result(device_id, result):\n",
    "    six_rate = result.six_count / result.roll_count\n",
    "    if six_rate > 0.3:\n",
    "        print(f\"Device #{device_id} was lucky! six-rate={six_rate*100:2.0f}%!!\")\n",
    "    else:\n",
    "        print(f\"Device #{device_id} is normal.\")\n",
    "\n",
    "\n",
    "for device_id, result in enumerate(bobbin.unshard(results)):\n",
    "    print_result(device_id, result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `gather_from_jax_processes`\n",
    "\n",
    "`gather_from_jax_processes` are important in multi-process environment.  In multi-process environment, sometimes one want to gather some metrics.\n",
    "In JIT-ed function, this is done by `allgather`.  `gather_from_jax_processes` is a short-cut for performing the similar operations in pure-python context\n",
    "(by essentially create a function that only does `allgather` and call it in a `pmap` context.)\n",
    "\n",
    "### `assert_replica_integrity`\n",
    "\n",
    "Similar to `gather_from_jax_processes` this is a short cut for checking integrity of the variables that are expected to be identical among the devices.\n",
    "This function essentially does `gather_from_jax_processes` on CPU backend, and compare the values from different devices and different processes, and raises an exception if there's mismatch.\n",
    "This operation is slow and should only be needed for debugging purpose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb4d849bafa02e801559e3934071dec087ae9bd4e6fac9af5ed0b4f8b22179c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to set up periodic action in the loop\n",
    "\n",
    "This notebook demonstrates `crontab` feature of bobbin."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble: Install prerequisites, import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade pip\n",
    "!pip -q install --upgrade \"jax[cpu]\"\n",
    "!pip -q uninstall -y bobbin\n",
    "!pip -q install --upgrade git+https://github.com/yotarok/bobbin.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 17:41:54.518752: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/google/home/yotaro/cuda/gpus/cuda_11_0/lib64\n",
      "2023-02-27 17:41:54.518868: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/google/home/yotaro/cuda/gpus/cuda_11_0/lib64\n",
      "2023-02-27 17:41:54.518876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import logging\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import bobbin\n",
    "import chex\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "Array = chex.Array\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout)\n",
    "logging.root.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.addHandler(logging.FileHandler(\"/dev/stdout\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tasks and models\n",
    "\n",
    "Here, we will demonstrate how to construct a loop that involves full training setup. Some training/ evaluation setup is needed.\n",
    "Only minimum explanation added to the training/ evaluation code below.  Please refer the following documents for training/ evaluation tasks in bobbin.\n",
    "\n",
    "- Training: [How to write a training loop](https://bobbin.readthedocs.io/en/latest/train_task.html)\n",
    "- Evaluation: [How to define an evaluation task](https://bobbin.readthedocs.io/en/latest/eval_task.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build a pipeline for pulling the training and evaluation datasets.\n",
    "The functions can be built as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(batch_size):\n",
    "    ds = tfds.load(\"mnist\", split=\"train\", as_supervised=True)\n",
    "    ds = ds.repeat().shuffle(1024).batch(batch_size).prefetch(1)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_eval_dataset(batch_size):\n",
    "    ds = tfds.load(\"mnist\", split=\"test[:1000]\", as_supervised=True)\n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    return ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the classifier model and loss function (in a subclass of `TrainTask`), as follows:\n",
    "(please also check [How to write a training loop](https://bobbin.readthedocs.io/en/latest/train_task.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistClassifier(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x: Array, *, training=True) -> Array:\n",
    "        batch_size, *unused_image_dims = x.shape\n",
    "        x = x.reshape((batch_size, -1))  # flatten the input image.\n",
    "        hidden = nn.sigmoid(nn.Dense(features=512)(x))\n",
    "        return nn.Dense(features=10)(hidden)\n",
    "\n",
    "\n",
    "class MnistTrainingTask(bobbin.TrainTask):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            MnistClassifier(),\n",
    "            example_args=(\n",
    "                np.zeros((1, 28, 28, 1), np.float32),  # comma-here is important\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, params, batch, *, extra_vars, prng_key, step):\n",
    "        images, labels = batch\n",
    "        logits = self.model.apply({\"params\": params}, images)\n",
    "        per_sample_loss = optax.softmax_cross_entropy(\n",
    "            logits=logits, labels=jax.nn.one_hot(labels, 10)\n",
    "        )\n",
    "        return jnp.mean(per_sample_loss), ({}, None)\n",
    "\n",
    "\n",
    "task = MnistTrainingTask()\n",
    "train_step_fn = task.make_training_step_fn().jit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metrics and how to evaluate the model can be defined as follows:\n",
    "(check [How to define an evaluation task](https://bobbin.readthedocs.io/en/latest/eval_task.html), too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset info from /usr/local/google/home/yotaro/tensorflow_datasets/mnist/3.0.1\n",
      "Fields info.[citation, splits, supervised_keys, module_name] from disk and from code do not match. Keeping the one from code.\n",
      "Reusing dataset mnist (/usr/local/google/home/yotaro/tensorflow_datasets/mnist/3.0.1)\n",
      "Constructing tf.data.Dataset mnist for split test[:1000], from /usr/local/google/home/yotaro/tensorflow_datasets/mnist/3.0.1\n"
     ]
    }
   ],
   "source": [
    "class EvalResults(bobbin.EvalResults):\n",
    "    correct_count: int\n",
    "    predict_count: int\n",
    "\n",
    "    @property\n",
    "    def accuracy(self) -> float:\n",
    "        return self.correct_count / self.predict_count\n",
    "\n",
    "    def is_better_than(self, other: \"EvalResults\") -> bool:\n",
    "        return self.accuracy > other.accuracy\n",
    "\n",
    "    def reduce(self, other: \"EvalResults\") -> \"EvalResults\":\n",
    "        return jax.tree_util.tree_map(lambda x, y: x + y, self, other)\n",
    "\n",
    "    def to_log_message(self) -> str:\n",
    "        return f\"formatted in `EvalResults.to_log_message`. acc={self.accuracy:.2f}\"\n",
    "\n",
    "\n",
    "class EvalTask(bobbin.EvalTask):\n",
    "    def __init__(self):\n",
    "        self.model = MnistClassifier()\n",
    "\n",
    "    def create_eval_results(self, dataset_name):\n",
    "        return EvalResults(correct_count=0, predict_count=0)\n",
    "\n",
    "    def evaluate(self, batch, model_vars) -> EvalResults:\n",
    "        inputs, labels = batch\n",
    "        logits = self.model.apply(model_vars, inputs)\n",
    "        predicts = logits.argmax(axis=-1)\n",
    "        return EvalResults(\n",
    "            correct_count=(predicts == labels).astype(np.int32).sum(),\n",
    "            predict_count=labels.shape[0],\n",
    "        )\n",
    "\n",
    "\n",
    "eval_batch_gens = {\n",
    "    \"test\": get_eval_dataset(32).as_numpy_iterator,\n",
    "}\n",
    "evaler = EvalTask()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup crontab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above models and tasks, we are now ready to actually write a training loop.\n",
    "As a first example, we design our main loop to greet to users for each 0.1 second using `CronTab.schedule` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset info from /usr/local/google/home/yotaro/tensorflow_datasets/mnist/3.0.1\n",
      "Fields info.[citation, splits, supervised_keys, module_name] from disk and from code do not match. Keeping the one from code.\n",
      "Reusing dataset mnist (/usr/local/google/home/yotaro/tensorflow_datasets/mnist/3.0.1)\n",
      "Constructing tf.data.Dataset mnist for split train, from /usr/local/google/home/yotaro/tensorflow_datasets/mnist/3.0.1\n",
      "Hello!! Training is currently at 1-th step. 1677497122.7034385\n",
      "Hello!! Training is currently at 27-th step. 1677497122.8050828\n",
      "Hello!! Training is currently at 76-th step. 1677497122.9070957\n",
      "Hello!! Training is currently at 136-th step. 1677497123.0086877\n",
      "Hello!! Training is currently at 197-th step. 1677497123.1097565\n",
      "Hello!! Training is currently at 257-th step. 1677497123.211207\n",
      "Hello!! Training is currently at 319-th step. 1677497123.312581\n",
      "Hello!! Training is currently at 380-th step. 1677497123.4127362\n",
      "Hello!! Training is currently at 416-th step. 1677497123.5137897\n",
      "Hello!! Training is currently at 477-th step. 1677497123.6141508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 20:25:23.648860: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "def say_hello(train_state, *, message: str, **kwargs):\n",
    "    print(\n",
    "        f\"{message} Training is currently at {train_state.step}-th step. {time.time()}\"\n",
    "    )\n",
    "\n",
    "\n",
    "crontab = bobbin.CronTab()\n",
    "crontab.schedule(say_hello, time_interval=0.1)\n",
    "\n",
    "prng_key = jax.random.PRNGKey(0)\n",
    "train_state = task.initialize_train_state(jax.random.PRNGKey(0), optax.sgd(0.01))\n",
    "for batch in get_train_dataset(64).take(500).as_numpy_iterator():\n",
    "    rng, prng_key = jax.random.split(prng_key)\n",
    "    train_state, step_info = train_step_fn(train_state, batch, rng)\n",
    "    crontab.run(train_state, message=\"Hello!!\", is_train_state_replicated=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument of `CronTab.schedule` is something called \"Action\" that can be anything called as `f(train_state, **kwargs)`.\n",
    "The action registered by `CronTab.schedule` will be called when you call `CronTab.run` at the end of each training step, and if the pre-specified condition met.\n",
    "In this case, the pre-defined condition is satisfied when the elapsed time since the action is lastly executed is longer than 0.1 second.\n",
    "(In other words, the action executed only once even if the step took longer than 0.2 seconds.)\n",
    "\n",
    "One can pass additional context information by adding keywords arguments to the call of `CronTab.run`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CronTab` is defined to be a hub for weakly connect the functionalities provided by other bobbin sub-modules.\n",
    "\n",
    "For example, `TrainTask` provides an action that write training log to the logger, and `EvalTask` provides an action to run the evaluation process over the datasets, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset info from /usr/local/google/home/yotaro/tensorflow_datasets/mnist/3.0.1\n",
      "Fields info.[citation, splits, supervised_keys, module_name] from disk and from code do not match. Keeping the one from code.\n",
      "Reusing dataset mnist (/usr/local/google/home/yotaro/tensorflow_datasets/mnist/3.0.1)\n",
      "Constructing tf.data.Dataset mnist for split train, from /usr/local/google/home/yotaro/tensorflow_datasets/mnist/3.0.1\n",
      "@step=100, loss=0.953003\n",
      "@step=123, loss=0.935717\n",
      "Start evaluation process over test\n",
      "Evaluation results for dataset=test @step=123\n",
      "formatted in `EvalResults.to_log_message`. acc=0.84\n",
      "@step=200, loss=0.647676\n",
      "Start evaluation process over test\n",
      "Evaluation results for dataset=test @step=246\n",
      "formatted in `EvalResults.to_log_message`. acc=0.89\n",
      "@step=300, loss=0.472512\n",
      "Start evaluation process over test\n",
      "Evaluation results for dataset=test @step=369\n",
      "formatted in `EvalResults.to_log_message`. acc=0.90\n",
      "@step=400, loss=0.521293\n",
      "Start evaluation process over test\n",
      "Evaluation results for dataset=test @step=492\n",
      "formatted in `EvalResults.to_log_message`. acc=0.90\n",
      "@step=500, loss=0.576384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 20:30:18.723999: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "crontab = bobbin.CronTab()\n",
    "crontab.schedule(\n",
    "    task.make_log_writer(loglevel=logging.WARNING), at_step=123, step_interval=100\n",
    ")\n",
    "crontab.schedule(\n",
    "    evaler.make_cron_action(eval_batch_gens, tensorboard_root_path=None),\n",
    "    step_interval=123,\n",
    ")\n",
    "prng_key = jax.random.PRNGKey(0)\n",
    "train_state = task.initialize_train_state(jax.random.PRNGKey(0), optax.sgd(0.01))\n",
    "for batch in get_train_dataset(64).take(500).as_numpy_iterator():\n",
    "    rng, prng_key = jax.random.split(prng_key)\n",
    "    train_state, step_info = train_step_fn(train_state, batch, rng)\n",
    "    crontab.run(train_state, step_info=step_info, is_train_state_replicated=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example, `TrainTask.make_log_writer` only writes a very simple log message, this can be customized by overriding `TrainTask.write_trainer_log` function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CronTab` can also be used for tying the training loop with checkpoint writers.  In the below example, we use two directory for storing checkpoints; one is for storing normal checkpoints for resuming the training processes, and the other one is for keeping best performing checkpoints for the future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset info from /usr/local/google/home/yotaro/tensorflow_datasets/mnist/3.0.1\n",
      "Fields info.[citation, splits, supervised_keys, module_name] from disk and from code do not match. Keeping the one from code.\n",
      "Reusing dataset mnist (/usr/local/google/home/yotaro/tensorflow_datasets/mnist/3.0.1)\n",
      "Constructing tf.data.Dataset mnist for split train, from /usr/local/google/home/yotaro/tensorflow_datasets/mnist/3.0.1\n",
      "Saving checkpoint at step: 1000\n",
      "Saved checkpoint at /tmp/tmppqxpbnaf/checkpoint_1000\n",
      "Start evaluation process over test\n",
      "Evaluation results for dataset=test @step=1000\n",
      "formatted in `EvalResults.to_log_message`. acc=0.90\n",
      "Saving checkpoint at step: 1000\n",
      "Saved checkpoint at /tmp/tmpf5gihi9h/checkpoint_1000\n",
      "Saving checkpoint at step: 2000\n",
      "Saved checkpoint at /tmp/tmppqxpbnaf/checkpoint_2000\n",
      "Removing checkpoint at /tmp/tmppqxpbnaf/checkpoint_1000\n",
      "Start evaluation process over test\n",
      "Evaluation results for dataset=test @step=2000\n",
      "formatted in `EvalResults.to_log_message`. acc=0.90\n",
      "Saving checkpoint at step: 3000\n",
      "Saved checkpoint at /tmp/tmppqxpbnaf/checkpoint_3000\n",
      "Removing checkpoint at /tmp/tmppqxpbnaf/checkpoint_2000\n",
      "Start evaluation process over test\n",
      "Evaluation results for dataset=test @step=3000\n",
      "formatted in `EvalResults.to_log_message`. acc=0.93\n",
      "Saving checkpoint at step: 3000\n",
      "Saved checkpoint at /tmp/tmpf5gihi9h/checkpoint_3000\n",
      "Removing checkpoint at /tmp/tmpf5gihi9h/checkpoint_1000\n",
      "Saving checkpoint at step: 4000\n",
      "Saved checkpoint at /tmp/tmppqxpbnaf/checkpoint_4000\n",
      "Removing checkpoint at /tmp/tmppqxpbnaf/checkpoint_3000\n",
      "Start evaluation process over test\n",
      "Evaluation results for dataset=test @step=4000\n",
      "formatted in `EvalResults.to_log_message`. acc=0.91\n",
      "Saving checkpoint at step: 5000\n",
      "Saved checkpoint at /tmp/tmppqxpbnaf/checkpoint_5000\n",
      "Removing checkpoint at /tmp/tmppqxpbnaf/checkpoint_4000\n",
      "Start evaluation process over test\n",
      "Evaluation results for dataset=test @step=5000\n",
      "formatted in `EvalResults.to_log_message`. acc=0.91\n",
      "Latest checkpoints:\n",
      "checkpoint_5000\n",
      "Best checkpoints:\n",
      "checkpoint_3000  results.json\n",
      "Performance of the best checkpoint\n",
      "{\"correct_count\": 925, \"predict_count\": 1000}"
     ]
    }
   ],
   "source": [
    "checkpoint_temp_dir = tempfile.TemporaryDirectory()\n",
    "best_checkpoint_temp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "crontab = bobbin.CronTab()\n",
    "crontab.schedule(\n",
    "    task.make_checkpoint_saver(checkpoint_temp_dir.name), step_interval=1000\n",
    ")\n",
    "crontab.schedule(\n",
    "    evaler.make_cron_action(\n",
    "        eval_batch_gens, tensorboard_root_path=None\n",
    "    ).keep_best_checkpoint(\"test\", best_checkpoint_temp_dir.name),\n",
    "    step_interval=1000,\n",
    ")\n",
    "\n",
    "prng_key = jax.random.PRNGKey(0)\n",
    "train_state = task.initialize_train_state(jax.random.PRNGKey(0), optax.sgd(0.1))\n",
    "for batch in get_train_dataset(64).take(5000).as_numpy_iterator():\n",
    "    rng, prng_key = jax.random.split(prng_key)\n",
    "    train_state, step_info = train_step_fn(train_state, batch, rng)\n",
    "    crontab.run(train_state, step_info=step_info, is_train_state_replicated=False)\n",
    "\n",
    "print(\"Latest checkpoints:\")\n",
    "!ls {checkpoint_temp_dir.name}\n",
    "print(\"Best checkpoints:\")\n",
    "!ls {best_checkpoint_temp_dir.name}\n",
    "print(\"Results of the best checkpoint\")\n",
    "!cat {best_checkpoint_temp_dir.name}/results.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb4d849bafa02e801559e3934071dec087ae9bd4e6fac9af5ed0b4f8b22179c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
